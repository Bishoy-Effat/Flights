{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "88cbac4b",
   "metadata": {},
   "outputs": [],
   "source": [
    "pip install psycopg2-binary"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "1c3f665b",
   "metadata": {},
   "outputs": [
    {
     "ename": "ModuleNotFoundError",
     "evalue": "No module named 'pyspark'",
     "output_type": "error",
     "traceback": [
      "\u001b[31m---------------------------------------------------------------------------\u001b[39m",
      "\u001b[31mModuleNotFoundError\u001b[39m                       Traceback (most recent call last)",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[1]\u001b[39m\u001b[32m, line 1\u001b[39m\n\u001b[32m----> \u001b[39m\u001b[32m1\u001b[39m \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34;01mpyspark\u001b[39;00m\u001b[34;01m.\u001b[39;00m\u001b[34;01msql\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mimport\u001b[39;00m SparkSession\n\u001b[32m      2\u001b[39m \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34;01mpyspark\u001b[39;00m\u001b[34;01m.\u001b[39;00m\u001b[34;01msql\u001b[39;00m\u001b[34;01m.\u001b[39;00m\u001b[34;01mfunctions\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mimport\u001b[39;00m from_json, col\n\u001b[32m      3\u001b[39m \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34;01mpyspark\u001b[39;00m\u001b[34;01m.\u001b[39;00m\u001b[34;01msql\u001b[39;00m\u001b[34;01m.\u001b[39;00m\u001b[34;01mtypes\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mimport\u001b[39;00m StructType, StringType, LongType\n",
      "\u001b[31mModuleNotFoundError\u001b[39m: No module named 'pyspark'"
     ]
    }
   ],
   "source": [
    "from pyspark.sql import SparkSession\n",
    "from pyspark.sql.functions import from_json, col\n",
    "from pyspark.sql.types import StructType, StringType, LongType\n",
    "import psycopg2\n",
    "\n",
    "spark = SparkSession.builder.appName(\"FlightStream\").getOrCreate()\n",
    "\n",
    "schema = StructType() \\\n",
    "    .add(\"flight_id\", StringType()) \\\n",
    "    .add(\"origin\", StringType()) \\\n",
    "    .add(\"destination\", StringType()) \\\n",
    "    .add(\"status\", StringType()) \\\n",
    "    .add(\"departure_time\", LongType()) \\\n",
    "    .add(\"arrival_time\", LongType())\n",
    "\n",
    "df = spark.readStream \\\n",
    "    .format(\"kafka\") \\\n",
    "    .option(\"kafka.bootstrap.servers\", \"broker:29092\") \\\n",
    "    .option(\"subscribe\", \"flights\") \\\n",
    "    .option(\"startingOffsets\", \"latest\") \\\n",
    "    .load()\n",
    "\n",
    "flights = df.select(from_json(col(\"value\").cast(\"string\"), schema).alias(\"data\")).select(\"data.*\")\n",
    "\n",
    "def foreach_batch(df, epoch_id):\n",
    "    conn = psycopg2.connect(\n",
    "        dbname=\"flights_project\", user=\"admin\", password=\"admin\", host=\"postgres_general\"\n",
    "    )\n",
    "    cur = conn.cursor()\n",
    "    for row in df.collect():\n",
    "        cur.execute(\"\"\"\n",
    "            INSERT INTO flights (flight_id, origin, destination, status, departure_time, arrival_time)\n",
    "            VALUES (%s,%s,%s,%s,%s,%s)\n",
    "            ON CONFLICT (flight_id) DO UPDATE\n",
    "            SET status = EXCLUDED.status, arrival_time = EXCLUDED.arrival_time;\n",
    "        \"\"\", (row.flight_id, str(row.origin), str(row.destination), row.status, row.departure_time, row.arrival_time))\n",
    "    conn.commit()\n",
    "    cur.close()\n",
    "    conn.close()\n",
    "\n",
    "query = flights.writeStream.foreachBatch(foreach_batch).start()\n",
    "query.awaitTermination()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "0294ab46",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Collecting pyspark\n",
      "  Using cached pyspark-4.0.1.tar.gz (434.2 MB)\n",
      "  Preparing metadata (setup.py): started\n",
      "  Preparing metadata (setup.py): finished with status 'done'\n",
      "Collecting py4j==0.10.9.9 (from pyspark)\n",
      "  Using cached py4j-0.10.9.9-py2.py3-none-any.whl.metadata (1.3 kB)\n",
      "Using cached py4j-0.10.9.9-py2.py3-none-any.whl (203 kB)\n",
      "Building wheels for collected packages: pyspark\n",
      "  Building wheel for pyspark (setup.py): started\n",
      "  Building wheel for pyspark (setup.py): still running...\n",
      "  Building wheel for pyspark (setup.py): finished with status 'done'\n",
      "  Created wheel for pyspark: filename=pyspark-4.0.1-py2.py3-none-any.whl size=434813900 sha256=9bff1f49108aa686e3cf23ef555912de86cf22de2eb62e70790bae6e9c19d29e\n",
      "  Stored in directory: c:\\users\\bisho\\appdata\\local\\pip\\cache\\wheels\\76\\f8\\dc\\9195b82b8586561710077d42370ae400e8a023af43052d1fec\n",
      "Successfully built pyspark\n",
      "Installing collected packages: py4j, pyspark\n",
      "\n",
      "   ---------------------------------------- 0/2 [py4j]\n",
      "   -------------------- ------------------- 1/2 [pyspark]\n",
      "   -------------------- ------------------- 1/2 [pyspark]\n",
      "   -------------------- ------------------- 1/2 [pyspark]\n",
      "   -------------------- ------------------- 1/2 [pyspark]\n",
      "   -------------------- ------------------- 1/2 [pyspark]\n",
      "   -------------------- ------------------- 1/2 [pyspark]\n",
      "   -------------------- ------------------- 1/2 [pyspark]\n",
      "   -------------------- ------------------- 1/2 [pyspark]\n",
      "   -------------------- ------------------- 1/2 [pyspark]\n",
      "   -------------------- ------------------- 1/2 [pyspark]\n",
      "   -------------------- ------------------- 1/2 [pyspark]\n",
      "   -------------------- ------------------- 1/2 [pyspark]\n",
      "   -------------------- ------------------- 1/2 [pyspark]\n",
      "   -------------------- ------------------- 1/2 [pyspark]\n",
      "   -------------------- ------------------- 1/2 [pyspark]\n",
      "   -------------------- ------------------- 1/2 [pyspark]\n",
      "   -------------------- ------------------- 1/2 [pyspark]\n",
      "   -------------------- ------------------- 1/2 [pyspark]\n",
      "   -------------------- ------------------- 1/2 [pyspark]\n",
      "   -------------------- ------------------- 1/2 [pyspark]\n",
      "   -------------------- ------------------- 1/2 [pyspark]\n",
      "   -------------------- ------------------- 1/2 [pyspark]\n",
      "   -------------------- ------------------- 1/2 [pyspark]\n",
      "   -------------------- ------------------- 1/2 [pyspark]\n",
      "   -------------------- ------------------- 1/2 [pyspark]\n",
      "   -------------------- ------------------- 1/2 [pyspark]\n",
      "   -------------------- ------------------- 1/2 [pyspark]\n",
      "   -------------------- ------------------- 1/2 [pyspark]\n",
      "   -------------------- ------------------- 1/2 [pyspark]\n",
      "   -------------------- ------------------- 1/2 [pyspark]\n",
      "   -------------------- ------------------- 1/2 [pyspark]\n",
      "   -------------------- ------------------- 1/2 [pyspark]\n",
      "   -------------------- ------------------- 1/2 [pyspark]\n",
      "   -------------------- ------------------- 1/2 [pyspark]\n",
      "   -------------------- ------------------- 1/2 [pyspark]\n",
      "   -------------------- ------------------- 1/2 [pyspark]\n",
      "   -------------------- ------------------- 1/2 [pyspark]\n",
      "   -------------------- ------------------- 1/2 [pyspark]\n",
      "   -------------------- ------------------- 1/2 [pyspark]\n",
      "   -------------------- ------------------- 1/2 [pyspark]\n",
      "   -------------------- ------------------- 1/2 [pyspark]\n",
      "   ---------------------------------------- 2/2 [pyspark]\n",
      "\n",
      "Successfully installed py4j-0.10.9.9 pyspark-4.0.1\n",
      "Note: you may need to restart the kernel to use updated packages.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  DEPRECATION: Building 'pyspark' using the legacy setup.py bdist_wheel mechanism, which will be removed in a future version. pip 25.3 will enforce this behaviour change. A possible replacement is to use the standardized build interface by setting the `--use-pep517` option, (possibly combined with `--no-build-isolation`), or adding a `pyproject.toml` file to the source tree of 'pyspark'. Discussion can be found at https://github.com/pypa/pip/issues/6334\n"
     ]
    }
   ],
   "source": [
    "pip install pyspark"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "optapy_env",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
